1.朴素贝叶斯
朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布）只是做了一堆计数。如果注有条件独立性假设（一个比较严格的条件），
朴素贝叶斯分类器的收敛速度将快于判别模型，如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。
1）优点：
朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
对小规模的数据表现很好，能个处理多分类任务，适合增量式训练；
对缺失数据不太敏感，算法也比较简单，常用于文本分类。
2）缺点：
需要计算先验概率；
分类决策存在错误率；
对输入数据的表达形式很敏感。

2.Logistic Regression（逻辑回归）
属于判别式模型，有很多正则化模型的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树与SVM机相比，
你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法，online gradient descent）。
如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去。
1）优点
实现简单，广泛的应用于工业问题上；
分类时计算量非常小，速度很快，存储资源低；
便利的观测样本概率分数；
对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；
2）缺点
当特征空间很大时，逻辑回归的性能不是很好；
容易欠拟合，一般准确度不太高
不能很好地处理大量多类特征或变量；
只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
对于非线性特征，需要进行转换；

3.线性回归
线性回归是用于回归的，而不像Logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，
优点： 实现简单，计算简单；
缺点： 不能拟合非线性数据

4.最近邻算法——KNN
主要过程为：
1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）
2. 对上面所有的距离值进行排序
3. 选前k个最小距离的样本
4. 根据这k个样本的标签进行投票，得到最后的分类类别
如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。
一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。
近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。
对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。
1）优点
理论成熟，思想简单，既可以用来做分类也可以用来做回归；
可用于非线性分类；
训练时间复杂度为O(n)；
对数据没有假设，准确度高，对outlier不敏感；
2）缺点
计算量大；
样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
需要大量的内存；

5.决策树
1）优点
计算简单，易于理解，可解释性强；
比较适合处理有缺失属性的样本；
能够处理不相关的特征；
在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
2）缺点
容易发生过拟合（随机森林可以很大程度上减少过拟合）；
忽略了数据之间的相关性；
对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）

6.Adaboost
Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。是一种典型的boosting算法。下面是总结下它的优缺点。
1)优点
adaboost是一种有很高精度的分类器。
可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。
简单，不用做特征筛选。
不容易发生overfitting。
2)缺点
对outlier比较敏感

7.SVM支持向量机
高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。
可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。
1)优点
可以解决高维问题，即大型特征空间；
能够处理非线性特征的相互作用；
无需依赖整个数据；
可以提高泛化能力；
2)缺点
当观测样本很多时，效率并不是很高；
对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
对缺失数据敏感；
对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：
第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；
第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。
对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。

8.人工神经网络
1)优点
分类的准确度高；
并行分布处理能力强,分布存储及学习能力强，
对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系；
具备联想记忆的功能。
2)缺点
神经络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；
不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；
学习时间过长,甚至可能达不到学习的目的。

9.K-Means聚类
1)优点
算法简单，容易实现 ；
对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。
算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。
2)缺点
对数据类型要求较高，适合数值型数据；
可能收敛到局部最小值，在大规模数据上收敛较慢
K值比较难以选取；
对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；
不适合于发现非凸面形状的簇，或者大小差别很大的簇。
对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

